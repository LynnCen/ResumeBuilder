# 最佳实践与开发规范

> **生产级 AI Agent 的开发指南**

---

## 一、代码规范

### 1.1 项目结构

```
agent-service/
├── app/
│   ├── core/              # 核心模块
│   │   ├── agent.py       # Agent 核心逻辑
│   │   ├── state.py       # 状态定义
│   │   └── checkpointer.py
│   ├── nodes/             # 节点实现
│   │   ├── llm_node.py
│   │   └── tools_node.py
│   ├── tools/             # 工具实现
│   │   ├── base.py        # 工具基类
│   │   ├── registry.py    # 工具注册表
│   │   └── image_gen.py
│   ├── api/               # API 层
│   │   ├── routes.py
│   │   └── models.py      # Pydantic 模型
│   ├── utils/             # 工具函数
│   │   ├── logging.py
│   │   └── metrics.py
│   └── config.py          # 配置
├── tests/                 # 测试
├── requirements.txt
├── Dockerfile
└── README.md
```

### 1.2 命名规范

**文件和模块**：
- 使用小写 + 下划线：`llm_node.py`
- 一个文件一个主要类或功能模块

**类**：
- 使用 PascalCase：`AgentService`, `LLMNode`
- 工具类加 `Tool` 后缀：`ImageGenTool`

**函数**：
- 使用 snake_case：`execute_tool()`, `generate_title()`
- 异步函数加 `async` 前缀或使用 `a` 前缀（可选）：`async def call_llm()` 或 `async def acall_llm()`

**变量**：
- 使用 snake_case：`thread_id`, `tool_call_id`
- 常量使用大写：`MAX_LLM_CALLS = 10`

### 1.3 类型注解

**必须使用类型注解**：

```python
from typing import List, Dict, Optional, Any

# ✅ 好的
async def call_llm(
    messages: List[Dict[str, Any]],
    model: str = "doubao-pro-128k",
    temperature: float = 0.7
) -> Dict[str, Any]:
    pass

# ❌ 不好的
async def call_llm(messages, model="doubao-pro-128k"):
    pass
```

### 1.4 文档字符串

```python
async def execute_tool(
    tool_call: Dict[str, Any],
    context: Dict[str, Any]
) -> Dict[str, Any]:
    """
    执行工具调用
    
    Args:
        tool_call: 工具调用信息，包含 name 和 arguments
        context: 上下文信息，包含 user_id, thread_id 等
    
    Returns:
        工具执行结果，格式：
        {
            "tool_name": str,
            "result": "success" | "failed",
            ...
        }
    
    Raises:
        ValueError: 工具不存在
        ToolExecutionError: 工具执行失败
    
    Example:
        >>> result = await execute_tool(
        ...     {"name": "generate_image", "arguments": {...}},
        ...     {"user_id": "user_123"}
        ... )
    """
    pass
```

---

## 二、性能优化

### 2.1 异步编程

**DO：使用异步**

```python
# ✅ 好的：并发执行
async def execute_multiple_tools(tool_calls: List[Dict]) -> List[Dict]:
    tasks = [execute_tool(call) for call in tool_calls]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results

# ❌ 不好的：串行执行
async def execute_multiple_tools(tool_calls: List[Dict]) -> List[Dict]:
    results = []
    for call in tool_calls:
        result = await execute_tool(call)
        results.append(result)
    return results
```

### 2.2 缓存策略

**工具描述缓存**：

```python
from functools import lru_cache

@lru_cache(maxsize=None)
def get_tool_descriptions(skill_id: int) -> str:
    """
    获取工具描述（带缓存）
    
    工具描述不会频繁变化，可以缓存
    """
    tools = get_available_tools(skill_id)
    return generate_all_tools_description(tools)
```

**System Prompt 缓存**：

```python
@lru_cache(maxsize=128)
def build_system_prompt(skill_id: int) -> str:
    """构造 System Prompt（带缓存）"""
    skill = get_skill_config(skill_id)
    tool_descs = get_tool_descriptions(skill_id)
    return skill.system_prompt + "\n\n" + FN_CALL_TEMPLATE.format(
        tool_descs=tool_descs
    )
```

### 2.3 连接池

**HTTP 连接池**：

```python
import httpx

# ✅ 好的：使用连接池
class LLMClient:
    def __init__(self):
        self.client = httpx.AsyncClient(
            timeout=30.0,
            limits=httpx.Limits(
                max_connections=100,
                max_keepalive_connections=20
            )
        )
    
    async def complete(self, ...):
        response = await self.client.post(...)
        return response

# ❌ 不好的：每次创建新连接
async def call_llm(...):
    async with httpx.AsyncClient() as client:
        response = await client.post(...)
```

**Redis 连接池**：

```python
import redis.asyncio as aioredis

# 创建连接池
redis_pool = aioredis.ConnectionPool.from_url(
    "redis://localhost:6379",
    max_connections=20
)

redis_client = aioredis.Redis(connection_pool=redis_pool)
```

---

## 三、安全建议

### 3.1 输入验证

```python
from pydantic import BaseModel, Field, validator

class ChatRequest(BaseModel):
    """聊天请求"""
    
    message: str = Field(..., max_length=10000)
    thread_id: Optional[str] = Field(None, max_length=100)
    user_id: str = Field(..., max_length=100)
    skill_id: int = Field(1, ge=1, le=100)
    
    @validator('message')
    def validate_message(cls, v):
        # 检查空白
        if not v.strip():
            raise ValueError("Message cannot be empty")
        
        # 检查特殊字符
        if any(c in v for c in ['<script>', '<iframe>']):
            raise ValueError("Invalid characters in message")
        
        return v.strip()
```

### 3.2 敏感信息处理

**不要在日志中记录敏感信息**：

```python
# ❌ 不好的
logger.info(f"User {user_id} logged in with password {password}")

# ✅ 好的
logger.info(f"User {user_id} logged in")
```

**不要在 Checkpoint 中存储敏感信息**：

```python
def save_checkpoint(state: AgentState):
    # 移除敏感信息
    safe_state = state.copy()
    if 'password' in safe_state:
        del safe_state['password']
    if 'api_key' in safe_state:
        del safe_state['api_key']
    
    checkpointer.put(safe_state)
```

### 3.3 内容审核

**Guardrails 集成**：

```python
async def check_content_safety(text: str) -> Dict[str, Any]:
    """内容安全检查"""
    
    # 调用内容审核服务
    result = await guardrails_service.check(text)
    
    if result['risk_level'] == 'high':
        return {
            "safe": False,
            "reason": "高风险内容",
            "action": "block"
        }
    
    elif result['risk_level'] == 'medium':
        return {
            "safe": True,
            "reason": "中风险内容",
            "action": "warn"
        }
    
    return {
        "safe": True,
        "reason": "安全",
        "action": "pass"
    }


async def llm_node(state: AgentState) -> dict:
    """LLM 节点"""
    
    # 调用 LLM
    response = await llm.complete(...)
    
    # 检查输出安全性
    safety_check = await check_content_safety(response)
    
    if not safety_check['safe'] and safety_check['action'] == 'block':
        # 中断并撤回
        raise NodeInterrupt(
            code=-1006,
            message="内容包含敏感信息",
            extra=safety_check
        )
    
    return {"messages": [response]}
```

---

## 四、成本控制

### 4.1 Token 优化

**控制上下文长度**：

```python
def trim_messages(messages: List[Dict], max_tokens: int = 4000) -> List[Dict]:
    """
    修剪消息历史，控制上下文长度
    
    策略：保留 system + 最近的 N 条消息
    """
    if not messages:
        return []
    
    # 1. 保留 system 消息
    system_messages = [msg for msg in messages if msg['role'] == 'system']
    other_messages = [msg for msg in messages if msg['role'] != 'system']
    
    # 2. 估算 token 数（粗略估计）
    def estimate_tokens(msg: Dict) -> int:
        content = msg['content']
        if isinstance(content, dict):
            content = json.dumps(content)
        return len(content) // 4  # 粗略估计
    
    # 3. 从最新消息开始累积
    kept_messages = []
    total_tokens = sum(estimate_tokens(msg) for msg in system_messages)
    
    for msg in reversed(other_messages):
        msg_tokens = estimate_tokens(msg)
        if total_tokens + msg_tokens > max_tokens:
            break
        kept_messages.append(msg)
        total_tokens += msg_tokens
    
    # 4. 恢复顺序
    return system_messages + list(reversed(kept_messages))
```

**使用更便宜的模型**：

```python
def select_model(task_complexity: str) -> str:
    """根据任务复杂度选择模型"""
    
    if task_complexity == "simple":
        return "doubao-lite"  # 便宜
    elif task_complexity == "medium":
        return "qwen-turbo"
    else:
        return "doubao-pro"  # 贵但质量高


def estimate_task_complexity(user_input: str) -> str:
    """估算任务复杂度"""
    
    # 简单规则
    if len(user_input) < 50 and not any(word in user_input for word in ['复杂', '详细', '深入']):
        return "simple"
    
    # TODO: 更复杂的判断逻辑
    
    return "medium"
```

### 4.2 调用次数限制

```python
MAX_LLM_CALLS = 10
MAX_TOOL_CALLS = 5

async def llm_node(state: AgentState) -> dict:
    """LLM 节点"""
    
    # 检查调用次数
    llm_calls = state.get("llm_calls", 0)
    if llm_calls >= MAX_LLM_CALLS:
        raise NodeInterrupt(
            code=-1003,
            message=f"LLM 调用次数超限（{MAX_LLM_CALLS}）"
        )
    
    # ... 调用 LLM ...
    
    return {
        "messages": [response],
        "llm_calls": llm_calls + 1
    }
```

### 4.3 监控和告警

```python
# 每日 Token 消耗告警
- alert: HighDailyTokenUsage
  expr: |
    sum(increase(agent_llm_tokens_total[24h]))
    > 10000000  # 1000万
  labels:
    severity: P2
  annotations:
    summary: "Daily token usage > 10M"

# 单用户 Token 消耗异常
- alert: UserHighTokenUsage
  expr: |
    sum by (user_id) (increase(agent_llm_tokens_total[1h]))
    > 100000  # 10万/小时
  labels:
    severity: P2
  annotations:
    summary: "User token usage > 100K/hour"
```

---

## 五、常见陷阱

### 5.1 状态污染

**❌ 错误：修改全局状态**

```python
# 全局变量
current_user_id = None

async def chat(user_input: str, user_id: str):
    global current_user_id
    current_user_id = user_id  # ❌ 污染全局状态
    
    # 在并发场景下会出现问题
    result = await agent.run(user_input)
    return result
```

**✅ 正确：状态隔离**

```python
async def chat(user_input: str, user_id: str):
    # 在 state 中传递
    state = {
        "user_id": user_id,
        "messages": [...]
    }
    
    result = await agent.run(state)
    return result
```

### 5.2 内存泄漏

**❌ 错误：无限增长的缓存**

```python
# 全局缓存，无过期机制
message_cache = {}

def cache_message(thread_id: str, message: Dict):
    if thread_id not in message_cache:
        message_cache[thread_id] = []
    message_cache[thread_id].append(message)  # ❌ 无限增长
```

**✅ 正确：使用 LRU 缓存或 TTL**

```python
from cachetools import TTLCache

# 带过期时间的缓存
message_cache = TTLCache(maxsize=1000, ttl=3600)  # 1小时过期
```

### 5.3 阻塞操作

**❌ 错误：在异步函数中使用同步 I/O**

```python
async def llm_node(state: AgentState) -> dict:
    # ❌ 同步 I/O，阻塞事件循环
    with open("config.json", "r") as f:
        config = json.load(f)
    
    response = await llm.complete(...)
    return {"messages": [response]}
```

**✅ 正确：使用异步 I/O**

```python
import aiofiles

async def llm_node(state: AgentState) -> dict:
    # ✅ 异步 I/O
    async with aiofiles.open("config.json", "r") as f:
        content = await f.read()
        config = json.loads(content)
    
    response = await llm.complete(...)
    return {"messages": [response]}
```

---

## 六、测试建议

### 6.1 单元测试

```python
import pytest
from unittest.mock import AsyncMock, MagicMock

@pytest.mark.asyncio
async def test_llm_node():
    """测试 LLM 节点"""
    
    # Mock LLM client
    mock_llm = AsyncMock()
    mock_llm.complete.return_value = "测试回复"
    
    # 准备状态
    state = {
        "messages": [
            {"role": "user", "content": "你好"}
        ],
        "llm_calls": 0
    }
    
    # 执行
    node = LLMNode(llm_client=mock_llm)
    result = await node.execute(state)
    
    # 断言
    assert result["llm_calls"] == 1
    assert len(result["messages"]) == 1
    assert result["messages"][0]["role"] == "assistant"
```

### 6.2 集成测试

```python
@pytest.mark.asyncio
async def test_full_chat_flow():
    """测试完整对话流程"""
    
    # 准备
    agent = create_test_agent()
    
    # 执行
    result = await agent.chat(
        user_input="帮我生成一张图片",
        thread_id="test_thread",
        user_id="test_user"
    )
    
    # 断言
    assert result["status"] == "success"
    assert "image_url" in result
```

### 6.3 压力测试

```python
import asyncio

async def stress_test(concurrent_requests: int = 100):
    """压力测试"""
    
    async def single_request():
        result = await agent.chat(
            user_input="你好",
            thread_id=f"thread_{uuid.uuid4()}",
            user_id="test_user"
        )
        return result
    
    # 并发请求
    tasks = [single_request() for _ in range(concurrent_requests)]
    
    start_time = time.time()
    results = await asyncio.gather(*tasks, return_exceptions=True)
    end_time = time.time()
    
    # 统计
    success_count = sum(1 for r in results if not isinstance(r, Exception))
    failed_count = len(results) - success_count
    avg_time = (end_time - start_time) / len(results)
    
    print(f"成功: {success_count}, 失败: {failed_count}, 平均时间: {avg_time:.2f}s")
```

---

## 七、总结

### 7.1 核心原则

1. **清晰的架构**：模块化、职责明确
2. **类型安全**：使用类型注解
3. **异步优先**：提升性能
4. **安全第一**：输入验证、内容审核
5. **成本意识**：控制 Token 消耗
6. **可观测性**：日志、指标、追踪
7. **测试覆盖**：单元测试、集成测试

### 7.2 Checklist

上线前检查清单：

- [ ] 代码通过 lint 检查
- [ ] 单元测试覆盖率 > 80%
- [ ] 集成测试通过
- [ ] 压力测试通过（满足性能指标）
- [ ] 安全审计完成
- [ ] 监控和告警配置完成
- [ ] 文档更新完成
- [ ] 成本预算评估完成

---

*文档版本：v1.0*  
*最后更新：2026-01-26*

**上一篇**：[← 可观测性](08-可观测性.md) | **返回**：[README](README.md)
